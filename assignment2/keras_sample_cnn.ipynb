{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_PATH = \"../data/cifar-10-batches-py/\"\n",
    "\n",
    "def unpack(fname):\n",
    "    import pickle\n",
    "    with open(fname, \"rb\") as fin:\n",
    "        dumped = pickle.load(fin, encoding=\"bytes\")\n",
    "    return dumped\n",
    "\n",
    "def reshape(data):\n",
    "    img = np.zeros((32, 32, 3), 'uint8')\n",
    "    img[..., 0] = np.reshape(data[:1024], (32, 32))\n",
    "    img[..., 1] = np.reshape(data[1024:2048], (32, 32))\n",
    "    img[..., 2] = np.reshape(data[2048:3072], (32, 32))\n",
    "    return img\n",
    "\n",
    "batches_meta = unpack(DATA_PATH + \"batches.meta\")\n",
    "data_batches = [\n",
    "    unpack(DATA_PATH + \"data_batch_\" + str(i+1))\n",
    "    for i in range(5)\n",
    "]\n",
    "test_batch = unpack(DATA_PATH + \"test_batch\")\n",
    "\n",
    "# keras这个例子的输入需要每张图是3维数组，用上面的reshape函数应该刚好可以整成他们要的格式\n",
    "for i in range(5):\n",
    "    data_batches[i][b\"data\"] = np.array([reshape(img) for img in data_batches[i][b\"data\"]])\n",
    "test_batch[b\"data\"] = np.array([reshape(img) for img in test_batch[b\"data\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (10000, 32, 32, 3)\n",
      "10000 train samples\n",
      "10000 test samples\n",
      "Using real-time data augmentation.\n",
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Epoch 1/100\n",
      "313/313 [==============================] - 33s 104ms/step - loss: 2.1184 - acc: 0.2065 - val_loss: 1.9298 - val_acc: 0.3017\n",
      "Epoch 2/100\n",
      "313/313 [==============================] - 32s 102ms/step - loss: 1.8932 - acc: 0.3107 - val_loss: 1.7694 - val_acc: 0.3716\n",
      "Epoch 3/100\n",
      "313/313 [==============================] - 32s 102ms/step - loss: 1.7979 - acc: 0.3420 - val_loss: 1.6757 - val_acc: 0.3979\n",
      "Epoch 4/100\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 1.7393 - acc: 0.3625 - val_loss: 1.6504 - val_acc: 0.3955\n",
      "Epoch 5/100\n",
      "313/313 [==============================] - 32s 103ms/step - loss: 1.6833 - acc: 0.3824 - val_loss: 1.5479 - val_acc: 0.4386\n",
      "Epoch 6/100\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 1.6391 - acc: 0.4001 - val_loss: 1.5081 - val_acc: 0.4535\n",
      "Epoch 7/100\n",
      "313/313 [==============================] - 32s 103ms/step - loss: 1.6018 - acc: 0.4173 - val_loss: 1.4852 - val_acc: 0.4606\n",
      "Epoch 8/100\n",
      "313/313 [==============================] - 32s 102ms/step - loss: 1.5518 - acc: 0.4341 - val_loss: 1.5205 - val_acc: 0.4578\n",
      "Epoch 9/100\n",
      "313/313 [==============================] - 32s 102ms/step - loss: 1.5291 - acc: 0.4462 - val_loss: 1.4990 - val_acc: 0.4577\n",
      "Epoch 10/100\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 1.4995 - acc: 0.4566 - val_loss: 1.4131 - val_acc: 0.4880\n",
      "Epoch 11/100\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 1.4778 - acc: 0.4632 - val_loss: 1.4038 - val_acc: 0.4950\n",
      "Epoch 12/100\n",
      "313/313 [==============================] - 31s 101ms/step - loss: 1.4522 - acc: 0.4761 - val_loss: 1.3172 - val_acc: 0.5285\n",
      "Epoch 13/100\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 1.4361 - acc: 0.4811 - val_loss: 1.2992 - val_acc: 0.5358\n",
      "Epoch 14/100\n",
      "313/313 [==============================] - 32s 101ms/step - loss: 1.4167 - acc: 0.4936 - val_loss: 1.2932 - val_acc: 0.5350\n",
      "Epoch 15/100\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 1.3847 - acc: 0.5057 - val_loss: 1.2925 - val_acc: 0.5425\n",
      "Epoch 16/100\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 1.3620 - acc: 0.5085 - val_loss: 1.2970 - val_acc: 0.5405\n",
      "Epoch 17/100\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 1.3507 - acc: 0.5145 - val_loss: 1.2365 - val_acc: 0.5575\n",
      "Epoch 18/100\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 1.3410 - acc: 0.5219 - val_loss: 1.3076 - val_acc: 0.5366\n",
      "Epoch 19/100\n",
      "313/313 [==============================] - 32s 101ms/step - loss: 1.3199 - acc: 0.5243 - val_loss: 1.2513 - val_acc: 0.5558\n",
      "Epoch 20/100\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 1.3010 - acc: 0.5338 - val_loss: 1.1966 - val_acc: 0.5789\n",
      "Epoch 21/100\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 1.2949 - acc: 0.5454 - val_loss: 1.2623 - val_acc: 0.5487\n",
      "Epoch 22/100\n",
      "313/313 [==============================] - 32s 101ms/step - loss: 1.2832 - acc: 0.5405 - val_loss: 1.1950 - val_acc: 0.5728\n",
      "Epoch 23/100\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 1.2600 - acc: 0.5505 - val_loss: 1.1572 - val_acc: 0.5922\n",
      "Epoch 24/100\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 1.2515 - acc: 0.5554 - val_loss: 1.1561 - val_acc: 0.5961\n",
      "Epoch 25/100\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 1.2366 - acc: 0.5626 - val_loss: 1.1627 - val_acc: 0.5849\n",
      "Epoch 26/100\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 1.2218 - acc: 0.5625 - val_loss: 1.1075 - val_acc: 0.6117\n",
      "Epoch 27/100\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 1.2102 - acc: 0.5706 - val_loss: 1.1400 - val_acc: 0.5986\n",
      "Epoch 28/100\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 1.1975 - acc: 0.5774 - val_loss: 1.1765 - val_acc: 0.5838\n",
      "Epoch 29/100\n",
      "313/313 [==============================] - 32s 101ms/step - loss: 1.1830 - acc: 0.5800 - val_loss: 1.1220 - val_acc: 0.6072\n",
      "Epoch 30/100\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 1.1712 - acc: 0.5816 - val_loss: 1.0768 - val_acc: 0.6227\n",
      "Epoch 31/100\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 1.1607 - acc: 0.5866 - val_loss: 1.0730 - val_acc: 0.6256\n",
      "Epoch 32/100\n",
      "313/313 [==============================] - 31s 101ms/step - loss: 1.1494 - acc: 0.5922 - val_loss: 1.0975 - val_acc: 0.6137\n",
      "Epoch 33/100\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 1.1506 - acc: 0.5922 - val_loss: 1.0968 - val_acc: 0.6142\n",
      "Epoch 34/100\n",
      "313/313 [==============================] - 32s 101ms/step - loss: 1.1418 - acc: 0.5885 - val_loss: 1.0506 - val_acc: 0.6328\n",
      "Epoch 35/100\n",
      "313/313 [==============================] - 32s 101ms/step - loss: 1.1246 - acc: 0.6018 - val_loss: 1.0652 - val_acc: 0.6315\n",
      "Epoch 36/100\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 1.1099 - acc: 0.6056 - val_loss: 1.0745 - val_acc: 0.6239\n",
      "Epoch 37/100\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 1.1031 - acc: 0.6064 - val_loss: 1.0197 - val_acc: 0.6451\n",
      "Epoch 38/100\n",
      "313/313 [==============================] - 34s 108ms/step - loss: 1.0901 - acc: 0.6112 - val_loss: 1.1752 - val_acc: 0.5972\n",
      "Epoch 39/100\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 1.0834 - acc: 0.6143 - val_loss: 1.0320 - val_acc: 0.6417\n",
      "Epoch 40/100\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 1.0834 - acc: 0.6135 - val_loss: 1.0394 - val_acc: 0.6309\n",
      "Epoch 41/100\n",
      "313/313 [==============================] - 31s 101ms/step - loss: 1.0644 - acc: 0.6227 - val_loss: 1.1457 - val_acc: 0.6050\n",
      "Epoch 42/100\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 1.0536 - acc: 0.6256 - val_loss: 1.0189 - val_acc: 0.6454\n",
      "Epoch 43/100\n",
      "313/313 [==============================] - 32s 101ms/step - loss: 1.0438 - acc: 0.6302 - val_loss: 0.9867 - val_acc: 0.6575\n",
      "Epoch 44/100\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 1.0478 - acc: 0.6287 - val_loss: 0.9857 - val_acc: 0.6592\n",
      "Epoch 45/100\n",
      "313/313 [==============================] - 32s 102ms/step - loss: 1.0385 - acc: 0.6323 - val_loss: 1.0440 - val_acc: 0.6361\n",
      "Epoch 46/100\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 1.0247 - acc: 0.6372 - val_loss: 1.0050 - val_acc: 0.6510\n",
      "Epoch 47/100\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 1.0111 - acc: 0.6382 - val_loss: 0.9931 - val_acc: 0.6545\n",
      "Epoch 48/100\n",
      "313/313 [==============================] - 33s 104ms/step - loss: 1.0217 - acc: 0.6381 - val_loss: 1.0000 - val_acc: 0.6526\n",
      "Epoch 49/100\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 1.0174 - acc: 0.6428 - val_loss: 0.9799 - val_acc: 0.6629\n",
      "Epoch 50/100\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 1.0113 - acc: 0.6456 - val_loss: 1.0199 - val_acc: 0.6422\n",
      "Epoch 51/100\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 0.9850 - acc: 0.6522 - val_loss: 0.9776 - val_acc: 0.6639\n",
      "Epoch 52/100\n",
      "313/313 [==============================] - 32s 104ms/step - loss: 0.9889 - acc: 0.6478 - val_loss: 0.9127 - val_acc: 0.6852\n",
      "Epoch 53/100\n",
      "313/313 [==============================] - 34s 110ms/step - loss: 0.9803 - acc: 0.6495 - val_loss: 0.9624 - val_acc: 0.6689\n",
      "Epoch 54/100\n",
      "313/313 [==============================] - 32s 102ms/step - loss: 0.9817 - acc: 0.6534 - val_loss: 0.9598 - val_acc: 0.6639\n",
      "Epoch 55/100\n",
      "313/313 [==============================] - 34s 107ms/step - loss: 0.9833 - acc: 0.6515 - val_loss: 0.9853 - val_acc: 0.6609\n",
      "Epoch 56/100\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 0.9622 - acc: 0.6573 - val_loss: 0.9611 - val_acc: 0.6638\n",
      "Epoch 57/100\n",
      "313/313 [==============================] - 34s 110ms/step - loss: 0.9802 - acc: 0.6508 - val_loss: 0.9197 - val_acc: 0.6824\n",
      "Epoch 58/100\n",
      "313/313 [==============================] - 33s 104ms/step - loss: 0.9572 - acc: 0.6583 - val_loss: 0.9918 - val_acc: 0.6556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "313/313 [==============================] - 35s 110ms/step - loss: 0.9496 - acc: 0.6613 - val_loss: 0.9209 - val_acc: 0.6803\n",
      "Epoch 60/100\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.9440 - acc: 0.6676 - val_loss: 0.9075 - val_acc: 0.6836\n",
      "Epoch 61/100\n",
      "313/313 [==============================] - 32s 103ms/step - loss: 0.9523 - acc: 0.6663 - val_loss: 0.8916 - val_acc: 0.6920\n",
      "Epoch 62/100\n",
      "313/313 [==============================] - 32s 101ms/step - loss: 0.9446 - acc: 0.6664 - val_loss: 0.9297 - val_acc: 0.6830\n",
      "Epoch 63/100\n",
      "313/313 [==============================] - 31s 101ms/step - loss: 0.9263 - acc: 0.6735 - val_loss: 0.9496 - val_acc: 0.6783\n",
      "Epoch 64/100\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 0.9335 - acc: 0.6707 - val_loss: 0.9788 - val_acc: 0.6633\n",
      "Epoch 65/100\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 0.9267 - acc: 0.6735 - val_loss: 0.8933 - val_acc: 0.6917\n",
      "Epoch 66/100\n",
      "313/313 [==============================] - 32s 101ms/step - loss: 0.9361 - acc: 0.6696 - val_loss: 0.9934 - val_acc: 0.6584\n",
      "Epoch 67/100\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 0.9154 - acc: 0.6788 - val_loss: 0.9129 - val_acc: 0.6853\n",
      "Epoch 68/100\n",
      "313/313 [==============================] - 32s 101ms/step - loss: 0.9134 - acc: 0.6723 - val_loss: 0.8941 - val_acc: 0.6897\n",
      "Epoch 69/100\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 0.9153 - acc: 0.6808 - val_loss: 0.9305 - val_acc: 0.6857\n",
      "Epoch 70/100\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 0.9105 - acc: 0.6760 - val_loss: 0.8880 - val_acc: 0.6941\n",
      "Epoch 71/100\n",
      "313/313 [==============================] - 31s 101ms/step - loss: 0.9131 - acc: 0.6755 - val_loss: 0.9229 - val_acc: 0.6847\n",
      "Epoch 72/100\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 0.9018 - acc: 0.6806 - val_loss: 0.8862 - val_acc: 0.6964\n",
      "Epoch 73/100\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 0.9005 - acc: 0.6838 - val_loss: 0.9083 - val_acc: 0.6868\n",
      "Epoch 74/100\n",
      "313/313 [==============================] - 32s 101ms/step - loss: 0.9019 - acc: 0.6842 - val_loss: 0.8816 - val_acc: 0.6974\n",
      "Epoch 75/100\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 0.8920 - acc: 0.6830 - val_loss: 0.8877 - val_acc: 0.6941\n",
      "Epoch 76/100\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 0.8973 - acc: 0.6865 - val_loss: 0.9045 - val_acc: 0.6854\n",
      "Epoch 77/100\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 0.8753 - acc: 0.6943 - val_loss: 0.9076 - val_acc: 0.6924\n",
      "Epoch 78/100\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 0.8781 - acc: 0.6919 - val_loss: 0.8993 - val_acc: 0.6920\n",
      "Epoch 79/100\n",
      "313/313 [==============================] - 32s 101ms/step - loss: 0.8778 - acc: 0.6880 - val_loss: 0.8621 - val_acc: 0.7033\n",
      "Epoch 80/100\n",
      "313/313 [==============================] - 35s 111ms/step - loss: 0.8761 - acc: 0.6931 - val_loss: 0.9279 - val_acc: 0.6869\n",
      "Epoch 81/100\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.8705 - acc: 0.6923 - val_loss: 0.9077 - val_acc: 0.6896\n",
      "Epoch 82/100\n",
      "313/313 [==============================] - 34s 108ms/step - loss: 0.8532 - acc: 0.7049 - val_loss: 0.8461 - val_acc: 0.7065\n",
      "Epoch 83/100\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.8734 - acc: 0.6943 - val_loss: 0.8538 - val_acc: 0.7067\n",
      "Epoch 84/100\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.8653 - acc: 0.6956 - val_loss: 0.8765 - val_acc: 0.7043\n",
      "Epoch 85/100\n",
      "313/313 [==============================] - 34s 109ms/step - loss: 0.8544 - acc: 0.7024 - val_loss: 0.9351 - val_acc: 0.6828\n",
      "Epoch 86/100\n",
      "313/313 [==============================] - 34s 108ms/step - loss: 0.8591 - acc: 0.6980 - val_loss: 0.9514 - val_acc: 0.6889\n",
      "Epoch 87/100\n",
      "313/313 [==============================] - 33s 104ms/step - loss: 0.8574 - acc: 0.7006 - val_loss: 0.8773 - val_acc: 0.7004\n",
      "Epoch 88/100\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.8513 - acc: 0.7027 - val_loss: 0.8689 - val_acc: 0.7042\n",
      "Epoch 89/100\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.8555 - acc: 0.7009 - val_loss: 0.8923 - val_acc: 0.6998\n",
      "Epoch 90/100\n",
      "313/313 [==============================] - 34s 108ms/step - loss: 0.8665 - acc: 0.6934 - val_loss: 0.9011 - val_acc: 0.6933\n",
      "Epoch 91/100\n",
      "313/313 [==============================] - 34s 108ms/step - loss: 0.8570 - acc: 0.7047 - val_loss: 0.8937 - val_acc: 0.7014\n",
      "Epoch 92/100\n",
      "313/313 [==============================] - 34s 108ms/step - loss: 0.8471 - acc: 0.7083 - val_loss: 0.8992 - val_acc: 0.6962\n",
      "Epoch 93/100\n",
      "313/313 [==============================] - 34s 108ms/step - loss: 0.8419 - acc: 0.7059 - val_loss: 0.8674 - val_acc: 0.7105\n",
      "Epoch 94/100\n",
      "313/313 [==============================] - 35s 112ms/step - loss: 0.8346 - acc: 0.7091 - val_loss: 0.8775 - val_acc: 0.7050\n",
      "Epoch 95/100\n",
      "313/313 [==============================] - 34s 108ms/step - loss: 0.8413 - acc: 0.7109 - val_loss: 0.9234 - val_acc: 0.6970\n",
      "Epoch 96/100\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.8380 - acc: 0.7100 - val_loss: 0.8956 - val_acc: 0.6951\n",
      "Epoch 97/100\n",
      "313/313 [==============================] - 34s 108ms/step - loss: 0.8379 - acc: 0.7086 - val_loss: 0.8902 - val_acc: 0.7019\n",
      "Epoch 98/100\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.8415 - acc: 0.7113 - val_loss: 0.9098 - val_acc: 0.7001\n",
      "Epoch 99/100\n",
      "313/313 [==============================] - 34s 108ms/step - loss: 0.8401 - acc: 0.7093 - val_loss: 0.8417 - val_acc: 0.7185\n",
      "Epoch 100/100\n",
      "228/313 [====================>.........] - ETA: 7s - loss: 0.8211 - acc: 0.7129"
     ]
    }
   ],
   "source": [
    "'''Train a simple deep CNN on the CIFAR10 small images dataset.\n",
    "It gets to 75% validation accuracy in 25 epochs, and 79% after 50 epochs.\n",
    "(it's still underfitting at that point, though).\n",
    "'''\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation = True ## 如果这里是true在训练的时候会自动对图像做一些微调增强训练效果\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5' ## 最后训练跑完模型会存到这里\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train) = data_batches[0][b\"data\"], data_batches[0][b\"labels\"] ## 这里我做了处理，官方例子会从网上下载cifar10，我喂了第一个batch\n",
    "(x_test, y_test) = test_batch[b\"data\"], test_batch[b\"labels\"]             ## 这里也一样\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
